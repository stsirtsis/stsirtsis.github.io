---
---

@string{conf = {Conference}}
@string{jour = {Journal}}
@string{pre = {Preprint}}

@article{straitouri2025narrowing,
      abbr=pre,
      title={Narrowing Action Choices with AI Improves Human Sequential Decisions}, 
      author={Straitouri, Eleni and Tsirtsis, Stratis and Artola Velasco, Ander and Gomez-Rodriguez, Manuel},
      journal={arXiv},
      year={2025},
      abstract={Recent work has shown that, in classification tasks, it is possible to design decision support systems that do not require human experts to understand when to cede agency to a classifier or when to exercise their own agency to achieve complementarity---experts using these systems make more accurate predictions than those made by the experts or the classifier alone. The key principle underpinning these systems reduces to adaptively controlling the level of human agency, by design. Can we use the same principle to achieve complementarity in sequential decision making tasks? In this paper, we answer this question affirmatively. We develop a decision support system that uses a pre-trained AI agent to narrow down the set of actions a human can take to a subset, and then asks the human to take an action from this action set. Along the way, we also introduce a bandit algorithm that leverages the smoothness properties of the action sets provided by our system to efficiently optimize the level of human agency. To evaluate our decision support system, we conduct a large-scale human subject study (n=1,600) where participants play a wildfire mitigation game. We find that participants who play the game supported by our system outperform those who play on their own by ∼30% and the AI agent used by our system by >2%, even though the AI agent largely outperforms participants playing without support. We have made available the data gathered in our human subject study as well as an open source implementation of our system at https://github.com/Networks-Learning/narrowing-action-choices.},
      link={https://arxiv.org/abs/2510.16097},
}


@article{velasco2025auditing,
      abbr=pre,
      title={Auditing Pay-Per-Token in Large Language Models}, 
      author={Artola Velasco, Ander and Tsirtsis, Stratis and Gomez-Rodriguez, Manuel},
      journal={arXiv},
      year={2025},
      abstract={Millions of users rely on a market of cloud-based services to obtain access to state-of-the-art large language models. However, it has been very recently shown that the de facto pay-per-token pricing mechanism used by providers creates a financial incentive for them to strategize and misreport the (number of) tokens a model used to generate an output. In this paper, we develop an auditing framework based on martingale theory that enables a trusted third-party auditor who sequentially queries a provider to detect token misreporting. Crucially, we show that our framework is guaranteed to always detect token misreporting, regardless of the provider's (mis-)reporting policy, and not falsely flag a faithful provider as unfaithful with high probability. To validate our auditing framework, we conduct experiments across a wide range of (mis-)reporting policies using several large language models from the Llama, Gemma and Ministral families, and input prompts from a popular crowdsourced benchmarking platform. The results show that our framework detects an unfaithful provider after observing fewer than ~70 reported outputs, while maintaining the probability of falsely flagging a faithful provider below α=0.05.},
      link={https://arxiv.org/abs/2510.05181},
}

@article{chatzi2025canonical,
      abbr=pre,
      title={Canonical Autoregressive Generation}, 
      author={Chatzi, Ivi and Corvelo Benz, Nina and Tsirtsis, Stratis and Gomez-Rodriguez, Manuel },
      journal={arXiv},
      year={2025},
      abstract={State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations--the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.},
      link={https://arxiv.org/abs/2506.06446},
}

@article{velasco2025llm,
      abbr=pre,
      title={Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives}, 
      author={Artola Velasco, Ander and Tsirtsis, Stratis and Okati, Nastaran and Gomez-Rodriguez, Manuel },
      journal={arXiv},
      year={2025},
      abstract={State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it -- they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we introduce an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, to completely eliminate the financial incentive to strategize, we introduce a simple incentive-compatible token pricing mechanism. Under this mechanism, the price users pay for an output provided by a model depends on the number of characters of the output -- they pay a fixed price per character. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama, Gemma and Ministral families, and input prompts from the LMSYS Chatbot Arena platform.},
      link={https://arxiv.org/abs/2505.21627},
}

@article{corvelo2025evaluation,
      abbr=pre,
      title={Evaluation of Large Language Models via Coupled Token Generation}, 
      author={Corvelo Benz, Nina and Tsirtsis, Stratis and Straitouri, Eleni and Chatzi, Ivi and Artola Velasco, Ander and Thejaswi, Suhas and Gomez-Rodriguez, Manuel },
      journal={arXiv},
      year={2025},
      abstract={State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large language models should control for the randomization underpinning their functioning. Our starting point is the development of a causal model for coupled autoregressive generation, which allows different large language models to sample responses with the same source of randomness. Building upon our causal model, we first show that, on evaluations based on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla autoregressive generation but using provably fewer samples. However, we further show that, on evaluations based on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly lead to different rankings when comparing more than two models, even with an infinite amount of samples. This suggests that the apparent advantage of a model over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process. To illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama family. We find that, across multiple knowledge areas from the popular MMLU benchmark dataset, coupled autoregressive generation requires up to 40% fewer samples to reach the same conclusions as vanilla autoregressive generation. Further, using data from the LMSYS Chatbot Arena platform, we find that the win-rates derived from pairwise comparisons by a strong large language model to prompts differ under coupled and vanilla autoregressive generation.},
      link={https://arxiv.org/abs/2502.01754},
      note={A preliminary version appeared at the <em>ICLR Workshop on Building Trust in LLMs</em>, 2025.}
}

@article{tsirtsis2019optimal,
  abbr=jour,
  title={Optimal Decision Making Under Strategic Behavior},
  author={Tsirtsis, Stratis and Tabibian, Behzad and Khajehnejad, Moein and Singla, Adish and Sch{\"o}lkopf, Bernhard and Gomez-Rodriguez, Manuel},
  journal={Management Science},
  year={2024},
  abstract={We are witnessing an increasing use of data-driven predictive models to inform decisions. As decisions have implications for individuals and society, there is increasing pressure on decision makers to be transparent about their decision policies. At the same time, individuals may use knowledge, gained by transparency, to invest effort strategically in order to maximize their chances of receiving a beneficial decision. Our goal is to find decision policies that are optimal in terms of utility in such a strategic setting. To this end, we first characterize how strategic investment of effort by individuals leads to a change in the feature distribution. Using this characterization, we first show that, in general, we cannot expect to find optimal decision policies in polynomial time and there are cases in which deterministic policies are suboptimal. Then, we demonstrate that, if the cost individuals pay to change their features satisfies a natural monotonicity assumption, we can narrow down the search for the optimal policy to a particular family of decision policies with a set of desirable properties, which allow for a highly effective polynomial time heuristic search algorithm using dynamic programming. Finally, under no assumptions on the cost individuals pay to change their features, we develop an iterative search algorithm that is guaranteed to find locally optimal decision policies also in polynomial time. Experiments on synthetic and real credit card data illustrate our theoretical findings and show that the decision policies found by our algorithms achieve higher utility than those that do not account for strategic behavior.},
  link={https://pubsonline.informs.org/doi/full/10.1287/mnsc.2021.02567},
  note={A preliminary version appeared at the <em>NeurIPS Workshop on Human-Centric Machine Learning</em>, 2019.},
  selected={true}
}

@article{chatzi2024counterfactual,
      abbr=conf,
      title={Counterfactual Token Generation in Large Language Models}, 
      author={Chatzi, Ivi and Corvelo Benz, Nina and Straitouri, Eleni  and Tsirtsis, Stratis  and Gomez-Rodriguez, Manuel },
      journal={4th Conference on Causal Learning and Reasoning (CLeaR)},
      year={2025},
      abstract={"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...] Lyra's eyes welled up with tears as she realized the bitter truth - she had sacrificed everything for fleeting riches, and lost the love of her crew, her family, and herself." Although this story, generated by a large language model, is captivating, one may wonder -- how would the story have unfolded if the model had chosen "Captain Maeve" as the protagonist instead? We cannot know. State-of-the-art large language models are stateless -- they maintain no internal memory or state. Given a prompt, they generate a sequence of tokens as an output using an autoregressive process. As a consequence, they cannot reason about counterfactual alternatives to tokens they have generated in the past. In this work, our goal is to enhance them with this functionality. To this end, we develop a causal model of token generation that builds upon the Gumbel-Max structural causal model. Our model allows any large language model to perform counterfactual token generation at almost no cost in comparison with vanilla token generation, it is embarrassingly simple to implement, and it does not require any fine-tuning nor prompt engineering. We implement our model on Llama 3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a quantitative analysis of counterfactually generated text. We conclude with a demonstrative application of counterfactual token generation for bias detection, unveiling interesting insights about the model of the world constructed by large language models.},
      link={https://proceedings.mlr.press/v275/chatzi25a.html},
      note={A preliminary version appeared at the <em>NeurIPS Workshop on Causality and Large Models</em>, 2024.},
      selected={true}
}

@article{tsirtsis2023finding,
  abbr=conf,
  title={Finding Counterfactually Optimal Action Sequences in Continuous State Spaces},
  author={Tsirtsis, Stratis and Gomez-Rodriguez, Manuel},
  journal={37th Conference on Neural Information Processing Systems (NeurIPS)},
  year={2023},
  abstract={Whenever a clinician reflects on the efficacy of a sequence of treatment decisions for a patient, they may try to identify critical time steps where, had they made different decisions, the patient's health would have improved. While recent methods at the intersection of causal inference and reinforcement learning promise to aid human experts, as the clinician above, to retrospectively analyze sequential decision making processes, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial time. Then, we develop a search method based on the A* algorithm that, under a natural form of Lipschitz continuity of the environment's dynamics, is guaranteed to return the optimal solution to the problem. Experiments on real clinical data show that our method is very efficient in practice, and it has the potential to offer interesting insights for sequential decision making tasks.},
  link={https://proceedings.neurips.cc/paper_files/paper/2023/hash/09ae6beae5f1ff38f05c05979097ea0f-Abstract-Conference.html},
  note={A preliminary version appeared at the <em>ICML Workshop on Counterfactuals in Minds and Machines</em>, 2023.},
}

@article{tsirtsis2024towards,
  abbr=conf,
  title={Towards a computational model of responsibility judgments in sequential human-AI collaboration},
  author={Tsirtsis, Stratis and Gomez-Rodriguez, Manuel and Gerstenberg, Tobias},
  journal={46th Annual Conference of the Cognitive Science Society (CogSci)},
  year={2024},
  abstract={When a human and an AI agent collaborate to complete a task and something goes wrong, who is responsible? Prior work has developed theories to describe how people assign responsibility to individuals in teams. However, there has been little work studying the cognitive processes that underlie responsibility judgments in human-AI collaborations, especially for tasks comprising a sequence of interdependent actions. In this work, we take a step towards filling this gap. Using semi-autonomous driving as a paradigm, we develop an environment that simulates stylized cases of human-AI collaboration using a generative model of agent behavior. We propose a model of responsibility that considers how unexpected an agent's action was, and what would have happened had they acted differently. We test the model's predictions empirically and find that in addition to action expectations and counterfactual considerations, participants' responsibility judgments are also affected by how much each agent actually contributed to the outcome.},
  link={https://escholarship.org/uc/item/5h1742zk},
  note={A preliminary version appeared at the <em>CHI Workshop on Theory of Mind in Human-AI Interaction</em>, 2024.},
}

@article{okati2023within,
  abbr=conf,
  title={On the Within-Group Fairness of Screening Classifiers},
  author={Okati, Nastaran and Tsirtsis, Stratis and Gomez-Rodriguez, Manuel},
  journal={40th International Conference on Machine Learning (ICML)},
  year={2023},
  abstract={Screening classifiers are increasingly used to identify qualified candidates in a variety of selection processes. In this context, it has been recently shown that if a classifier is calibrated, one can identify the smallest set of candidates which contains, in expectation, a desired number of qualified candidates using a threshold decision rule. This lends support to focusing on calibration as the only requirement for screening classifiers. In this paper, we argue that screening policies that use calibrated classifiers may suffer from an understudied type of within-group unfairness—they may unfairly treat qualified members within demographic groups of interest. Further, we argue that this type of unfairness can be avoided if classifiers satisfy within-group monotonicity, a natural monotonicity property within each group. Then, we introduce an efficient post-processing algorithm based on dynamic programming to minimally modify a given calibrated classifier so that its probability estimates satisfy within-group monotonicity. We validate our algorithm using US Census survey data and show that withingroup monotonicity can often be achieved at a small cost in terms of prediction granularity and shortlist size.},
  link={https://proceedings.mlr.press/v202/okati23a.html},
  note={Also appeared at the <em>2nd ACM SIGKDD Workshop on Ethical Artificial Intelligence: Methods and Applications</em>, 2023.}
}

@article{lorch2022quantifying,
  abbr=jour,
  title={Quantifying the Effects of Contact Tracing, Testing, and Containment Measures in the Presence of Infection Hotspots},
  author={Lorch, Lars and Kremer, Heiner and Trouleau, William and Tsirtsis, Stratis and Szanto, Aron and Sch{\"o}lkopf, Bernhard and Gomez-Rodriguez, Manuel},
  journal={ACM Transactions on Spatial Algorithms and Systems},
  year={2022},
  abstract={Multiple lines of evidence strongly suggest that infection hotspots, where a single individual infects many others, play a key role in the transmission dynamics of COVID-19. However, most of the existing epidemiological models fail to capture this aspect by neither representing the sites visited by individuals explicitly nor characterizing disease transmission as a function of individual mobility patterns. In this work, we introduce a temporal point process modeling framework that specifically represents visits to the sites where individuals get in contact and infect each other. Under our model, the number of infections caused by an infectious individual naturally emerges to be overdispersed. Using an efficient sampling algorithm, we demonstrate how to estimate the transmission rate of infectious individuals at the sites they visit and in their households using Bayesian optimization (BO) and longitudinal case data. Simulations using fine-grained and publicly available demographic data and site locations from Bern, Switzerland showcase the flexibility of our framework. To facilitate research and analyses of other cities and regions, we release an open-source implementation of our framework.},
  link={https://dl.acm.org/doi/10.1145/3530774}
}

@article{tsirtsis2022pooled,
  abbr=jour,
  title={Pooled Testing of Traced Contacts Under Superspreading Dynamics},
  author={Tsirtsis, Stratis and De, Abir and Lorch, Lars and Gomez-Rodriguez, Manuel},
  journal={PLOS Computational Biology},
  year={2022},
  abstract={Testing is recommended for all close contacts of confirmed COVID-19 patients. However, existing pooled testing methods are oblivious to the circumstances of contagion provided by contact tracing. Here, we build upon a well-known semi-adaptive pooled testing method, Dorfman’s method with imperfect tests, and derive a simple pooled testing method based on dynamic programming that is specifically designed to use information provided by contact tracing. Experiments using a variety of reproduction numbers and dispersion levels, including those estimated in the context of the COVID-19 pandemic, show that the pools found using our method result in a significantly lower number of tests than those found using Dorfman’s method. Our method provides the greatest competitive advantage when the number of contacts of an infected individual is small, or the distribution of secondary infections is highly overdispersed. Moreover, it maintains this competitive advantage under imperfect contact tracing and significant levels of dilution.},
  link={https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010008}
}

@article{tsirtsis2021counterfactual,
  abbr=conf,
  title={Counterfactual Explanations in Sequential Decision Making Under Uncertainty},
  author={Tsirtsis, Stratis and De, Abir and Gomez-Rodriguez, Manuel},
  journal={35th Conference on Neural Information Processing Systems (NeurIPS)},
  year={2021},
  abstract={Methods to find counterfactual explanations have predominantly focused on one-step decision making processes. In this work, we initiate the development of methods to find counterfactual explanations for decision making processes in which multiple, dependent actions are taken sequentially over time. We start by formally characterizing a sequence of actions and states using finite horizon Markov decision processes and the Gumbel-Max structural causal model. Building upon this characterization, we formally state the problem of finding counterfactual explanations for sequential decision making processes. In our problem formulation, the counterfactual explanation specifies an alternative sequence of actions differing in at most k actions from the observed sequence that could have led the observed process realization to a better outcome. Then, we introduce a polynomial time algorithm based on dynamic programming to build a counterfactual policy that is guaranteed to always provide the optimal counterfactual explanation on every possible realization of the counterfactual environment dynamics. We validate our algorithm using both synthetic and real data from cognitive behavioral therapy and show that the counterfactual explanations our algorithm finds can provide valuable insights to enhance sequential decision making under uncertainty.},
  link={https://proceedings.neurips.cc/paper/2021/hash/fd0a5a5e367a0955d81278062ef37429-Abstract.html},
  note={A preliminary version appeared at the <em>ICML Workshop on Interpretable Machine Learning in Healthcare</em>, 2021.},
  selected={true}
}

@article{finocchiaro2021bridging,
  abbr=conf,
  title={Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness},
  author={Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis},
  journal={4th ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
  year={2021},
  abstract={Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.},
  link={https://dl.acm.org/doi/10.1145/3442188.3445912},
  note={A preliminary version appeared at the <em>Harvard CRCS Workshop on AI for Social Good (AI4SG)</em>, 2020.}
}

@article{tsirtsis2020decisions,
  abbr=conf,
  title={Decisions, Counterfactual Explanations and Strategic Behavior},
  author={Tsirtsis, Stratis and Gomez-Rodriguez, Manuel},
  journal={34th Conference on Neural Information Processing Systems (NeurIPS)},
  year={2020},
  abstract={As data-driven predictive models are increasingly used to inform decisions, it has been argued that decision makers should provide explanations that help individuals understand what would have to change for these decisions to be beneficial ones. However, there has been little discussion on the possibility that individuals may use the above counterfactual explanations to invest effort strategically and maximize their chances of receiving a beneficial decision. In this paper, our goal is to find policies and counterfactual explanations that are optimal in terms of utility in such a strategic setting. We first show that, given a pre-defined policy, the problem of finding the optimal set of counterfactual explanations is NP-hard. Then, we show that the corresponding objective is nondecreasing and satisfies submodularity and this allows a standard greedy algorithm to enjoy approximation guarantees. In addition, we further show that the problem of jointly finding both the optimal policy and set of counterfactual explanations reduces to maximizing a non-monotone submodular function. As a result, we can use a recent randomized algorithm to solve the problem, which also offers approximation guarantees. Finally, we demonstrate that, by incorporating a matroid constraint into the problem formulation, we can increase the diversity of the optimal set of counterfactual explanations and incentivize individuals across the whole spectrum of the population to self improve. Experiments on synthetic and real lending and credit card data illustrate our theoretical findings and show that the counterfactual explanations and decision policies found by our algorithms achieve higher utility than several competitive baselines.},
  link={https://proceedings.neurips.cc/paper/2020/hash/c2ba1bc54b239208cb37b901c0d3b363-Abstract.html},
  note={A preliminary version appeared at the <em>4th Workshop on Mechanism Design for Social Good (MD4SG)</em>, 2020.},
}
